{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c872c679-c338-4914-8755-d952d75bba41",
   "metadata": {},
   "source": [
    "# 监督学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c190c8-b11b-4da7-8eec-cfa711defccb",
   "metadata": {},
   "source": [
    "X->y Learn from being given the \"right answers\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4351e1d6-2bc3-4e12-aa71-7573faf9df75",
   "metadata": {},
   "source": [
    "## 回归Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb0f1ae-3689-44a8-9f99-7e9b38a53ff2",
   "metadata": {},
   "source": [
    "从无数可能的数字预测出一个数字\n",
    "predict numbers & large number possible outputs\n",
    "房价预测\n",
    "判断垃圾邮件\n",
    "预测用户是否点击广告"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190bccf7-3dce-4f3e-8d72-a13082403523",
   "metadata": {},
   "source": [
    "### 线性回归模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356a3c2c-a07d-4830-8fbb-519f1a1f4278",
   "metadata": {},
   "source": [
    "![img.png](img.png)\n",
    "$\\hat{y}$:预测值 $y$:实际值\n",
    "$$f_{W,b}(x) = Wx+b$$\n",
    "单个变量(feature)的线性回归，需要找到对应的W和b能很好的拟合数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea5e38c-aaa0-4574-bacb-17b452ef5ebe",
   "metadata": {},
   "source": [
    "#### 重点：构建cost function(代价函数)\n",
    "\n",
    "##### 定义\n",
    "\n",
    "$$J{(w,b)} = \\frac{1}{2m} \\sum_{i=1}^{m} (y^{(i)} - f_{W,b}(x))^2 $$\n",
    "or\n",
    "$$J{(w,b)} = \\frac{1}{2m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 $$\n",
    "m:训练集数据量\n",
    "损失函数用来衡量模型预测结果与实际结果之间差异的函数。其值越小，代表模型预测结果越接近实际结果。在训练模型时，需要通过不断调整模型参数来最小化损失函数的值。\n",
    "\n",
    "##### 直觉\n",
    "\n",
    "$$minimizeJ(w,b) \\atop{w,b}$$\n",
    "Your goal is to find a model $$f_{w,b}(x) = wx + b$$, with parameters  $w,b$,\n",
    "which will accurately predict house values given an input $x$.\n",
    "The cost is a measure of how accurate the model is on the training data.\n",
    "\n",
    "```python \n",
    "def compute_cost(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the cost function for linear regression.\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray (m,)): Data, m examples \n",
    "      y (ndarray (m,)): target values\n",
    "      w,b (scalar)    : model parameters  \n",
    "    \n",
    "    Returns\n",
    "        total_cost (float): The cost of using w,b as the parameters for linear regression\n",
    "               to fit the data points in x and y\n",
    "    \"\"\"\n",
    "    # number of training examples\n",
    "    m = x.shape[0]\n",
    "    cost_sum = 0\n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        cost = (f_wb - y[i]) ** 2\n",
    "        cost_sum = cost_sum + cost\n",
    "    total_cost = (1 / (2 * m)) * cost_sum\n",
    "\n",
    "    return total_cost\n",
    "\n",
    "``` \n",
    "\n",
    "![img_1.png](img_1.png)\n",
    "\n",
    "```python\n",
    "\n",
    "x_train = np.array([1.0, 1.7, 2.0, 2.5, 3.0, 3.2])\n",
    "y_train = np.array([250, 300, 480, 430, 630, 730, ])\n",
    "\n",
    "plt.close('all')\n",
    "fig, ax, dyn_items = plt_stationary(x_train, y_train)\n",
    "updater = plt_update_onclick(fig, ax, x_train, y_train, dyn_items)\n",
    "```\n",
    "\n",
    "$$f_{w}(x) = wx$$\n",
    "\n",
    "$$f_{w,b}(x) = wx + b$$\n",
    "w,b自变量。所以是三维图形，可以取切面最密集的点就是拟合最好的w和b\n",
    "![img_2.png](img_2.png)\n",
    "\n",
    "### 梯度下降算法Gradient descent algorithm\n",
    "\n",
    "本质上是通过对当前$w,b$所在的位置进行修正，找到(1)式最低点的算法。  \n",
    "So far in this course, you have developed a linear model that predicts $f_{w,b}(x^{(i)})$:\n",
    "$$f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{1}$$\n",
    "In linear regression, you utilize input training data to fit the parameters $w$,$b$ by minimizing\n",
    "a measure of the error between our predictions $f_{w,b}(x^{(i)})$ and the actual data $y^{(i)}$.\n",
    "The measure is called the $cost$, $J(w,b)$.\n",
    "In training, you measure the cost over all of our training samples $x^{(i)},y^{(i)}$\n",
    "$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2\\tag{2}$$\n",
    "重复直到收敛\n",
    "$$\\begin{align*} \\lbrace \\newline\n",
    "\\; w &= w - \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{3} \\; \\newline\n",
    "b &= b - \\alpha \\frac{\\partial J(w,b)}{\\partial b} \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "$\\alpha$：0,1之间。代表梯度下降的速率(learning rate)。越大越激进\n",
    "\n",
    "1.重复更新w,b的值直到算法收敛，即w,b每次更新都不会发生太大变化了\n",
    "\n",
    "2.$w,b$同时更新，异步更新可能会造成问题\n",
    "\n",
    "Where parameters$w$, $b$ are updated simultaneously.  \n",
    "The gradient is defined as:\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial J(w,b)}{\\partial w} &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\\n",
    "\\frac{\\partial J(w,b)}{\\partial b} &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{5}\\\\\n",
    "\\end{align}$$\n",
    "![img_3.png](img_3.png)\n",
    "![img_4.png](img_4.png)\n",
    "3.越接近local minimum\n",
    "-  $$\\frac{\\partial J(w,b)}{\\partial w}$$Derivative becomes smaller\n",
    "- $\\delta{w}$Update steps become smaller\n",
    "$\\alpha$过大或过小造成的影响\n",
    "过大：达不到最低点，无法收敛(converge) 甚至发散(diverge)\n",
    "过小：步骤多且耗时\n",
    "![img_5.png](img_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a22ae42-9e5d-43fe-a246-f29c5c8054b4",
   "metadata": {},
   "source": [
    "### 多元线性回归Multiple Variable Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757d3218-b479-4e24-b8c3-c0fbf7282d20",
   "metadata": {},
   "source": [
    "#### 向量化Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef422fb9-4800-4573-9fc3-474dd86ecf86",
   "metadata": {},
   "source": [
    "向量化:np.dot(w,x) + b\n",
    "No 向量化(循环计算):$$f = w[0] * x[0] + w[1] * x[1] + w[2] * x[2] + b$$\n",
    "\n",
    "\n",
    "![img_6.png](img_6.png)\n",
    "使用向量计算dot而非循环计算的好处：利用并行计算提升效率\n",
    "\n",
    "![img_7.gif](img_7.gif)\n",
    "<center>点乘过程</center>\n",
    "\n",
    "![img_8.png](img_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc02fcb0-ff4e-4999-8666-9d9f934c5369",
   "metadata": {},
   "source": [
    "#### 多类特征Multiple features\n",
    "单个特征—>多个特征:$x->x_1,x_2,...$  \n",
    "占地面积->占地面积，卧室数量...  \n",
    "使用list存储  \n",
    "![](img_9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247e1963-c0c4-4f52-b13e-a277c350b2e6",
   "metadata": {},
   "source": [
    "examples are stored in a NumPy matrix X_train. Each row of the matrix represents one example. When you have $m$\n",
    "training examples ( $m$ is three in our example), and there are $n$ features (four in our example), $\\mathbf{X}$\n",
    "is a matrix with dimensions ($m$, $n$) (m rows, n columns).\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \n",
    "\\begin{pmatrix}\n",
    " x^{(0)}_0 & x^{(0)}_1 & \\cdots & x^{(0)}_{n-1} \\\\ \n",
    " x^{(1)}_0 & x^{(1)}_1 & \\cdots & x^{(1)}_{n-1} \\\\\n",
    " \\cdots \\\\\n",
    " x^{(m-1)}_0 & x^{(m-1)}_1 & \\cdots & x^{(m-1)}_{n-1} \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "notation:\n",
    "- $\\mathbf{x}^{(i)}$ is vector containing example i. $\\mathbf{x}^{(i)} = (x^{(i)}_0, x^{(i)}_1, \\cdots,x^{(i)}_\n",
    "{n-1})$\n",
    "- $x^{(i)}_j$ is element j in example i. The superscript in parenthesis indicates the example number while the\n",
    "subscript represents an element."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfa8d70-6f30-4a83-bd78-0f960dcf705e",
   "metadata": {},
   "source": [
    "#### 学习率learning rate\n",
    "![](img_10.png)\n",
    "![](img_11.png)\n",
    "\n",
    "$\\alpha$控制了参数更新的速率\n",
    "\n",
    "![](img_12.png)\n",
    "\n",
    "![](img_13.png)\n",
    "\n",
    "提高学习率$\\alpha$和增加迭代次数，作用都不大，如何解决？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a0c3af-4fff-4df9-acb3-491f9d8f8ac0",
   "metadata": {},
   "source": [
    "#### 特征缩放Feature Scaling\n",
    "        重新缩放数据集，使特征具有相似的范围。标准化特征，让特征分布均匀。\n",
    "![](img_14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba50350-265a-4373-87a2-8b39b69685b6",
   "metadata": {},
   "source": [
    "##### 如何发现特征需要被缩放\n",
    "![](img_15.png)\n",
    "\n",
    "如上图，$w_0$只需要10次迭代就收敛了，而其他参数需要迭代非常多次  \n",
    "造成这个现象的原因是$x_0$(平方英寸，千级别)非常的大，是别的feature的上千倍，所以乘以相同的倍数，该feature迭代梯度远大于其他，速度更快。  \n",
    "上图展示了w更新不均匀的原因。   \n",
    "  $\\alpha$由所有参数更新（w和b）共享。  \n",
    "  常见错误项乘以$w$的特征。  \n",
    "  特征的大小差异很大，使得一些特征的更新速度比其他特征快得多。在这种情况下，$w_0$乘以“大小（平方英尺）”，通常>1000，而w_1乘以“卧室数量”，通常为2-4。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59199dca-c2c4-47f9-b2fb-ce9384a5e573",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T13:32:31.812157Z",
     "iopub.status.busy": "2025-01-01T13:32:31.811554Z",
     "iopub.status.idle": "2025-01-01T13:32:31.821368Z",
     "shell.execute_reply": "2025-01-01T13:32:31.820266Z",
     "shell.execute_reply.started": "2025-01-01T13:32:31.812108Z"
    }
   },
   "source": [
    "##### 三种技巧\n",
    "\n",
    "- 特征缩放, 让每个特征除以人为规定的值，结果落在-1和1之间。\n",
    "- 均值归一化Mean normalization: $$x_i := \\dfrac{x_i - \\mu_i}{max - min} $$ \n",
    "- z-score归一化 Z-score normalization，所有特征的均值为0，标准差为1。\n",
    "$$x^{(i)}_j = \\dfrac{x^{(i)}_j - \\mu_j}{\\sigma_j} \\tag 4$$\n",
    "where $j$ selects a feature or a column in the X matrix. $µ_j$ is the mean of all the values for feature (j) and $\\sigma_j$ is the standard deviation of feature (j).\n",
    "$$\\begin{align}\n",
    "\\mu_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} x^{(i)}_j \\tag{5}\\\\\n",
    "\\sigma^2_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} (x^{(i)}_j - \\mu_j)^2  \\tag{6}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b934399-648a-4cfd-828c-9dbe0966ebc4",
   "metadata": {},
   "source": [
    "#### 多元线性回归model\n",
    "  $$f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\tag{1}$$\n",
    "  or in vector notation:\n",
    "  $$f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b  \\tag{2} $$\n",
    "  where $$\\cdot$$ is a vector dot product(向量乘)\n",
    "  To demonstrate the dot product, we will implement prediction using (1) and (2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5426937-341f-417f-8d49-1421bd6a9deb",
   "metadata": {},
   "source": [
    "#### 线性回归的评价指标\n",
    "\n",
    "**均方误差（Mean Squared Error，MSE）**：\n",
    "\n",
    "MSE 是模型预测值与真实值差的平方的平均值。它强调大的误差。\n",
    "\n",
    "$$\\begin{align}MSE = \\frac{\\Sigma (y_i - \\hat{y}_i)^2}{n}\\end{align}$$\n",
    "\n",
    "适用场景：适用于回归任务中，尤其是对大的预测误差更加敏感的场景。\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e557a3-9777-4d74-a369-4fd20cc25a7b",
   "metadata": {},
   "source": [
    "    \n",
    "**均方根误差（Root Mean Squared Error，RMSE）**：\n",
    "\n",
    "RMSE 是 MSE 的平方根，用于将误差带回与原始目标变量相同的量纲\n",
    "\n",
    "$$\\begin{align} RMSE = \\sqrt{\\frac{\\Sigma (y_i - \\hat{y}_i)^2}{n}} \\end{align}$$\n",
    "\n",
    "适用场景：与 MSE 类似，但 RMSE 更直观，误差与目标变量的尺度一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c44047-ce29-496f-a32e-403a1daa9138",
   "metadata": {},
   "source": [
    "**平均绝对误差（Mean Absolute Error，MAE）**：\n",
    "\n",
    "MAE 是预测值与真实值的绝对差的平均值，较少受大误差的影响。\n",
    "\n",
    "$$\\begin{align}MAE = \\frac{\\Sigma |y_i - \\hat{y}_i|}{n}\\end{align}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615982f0-32ba-4df7-ae5e-9b0185191017",
   "metadata": {},
   "source": [
    "**决定系数（Coefficient of Determination，R-squared，R²）**：\n",
    "R²度量了线性回归模型对数据的拟合程度。它的取值范围在0到1之间，越接近1表示模型拟合得越好。\n",
    "适用场景：用于评估回归模型的解释能力。\n",
    "\n",
    "$$\\begin{align}R² = 1 - \\frac{\\Sigma (yi - ŷi)²}{ \\Sigma (yi - ȳ)²}\\end{align}$$\n",
    "其中 yi 表示实际观测值，ŷi 表示模型的预测值，ȳ 表示实际观测值的均值。\n",
    "对于线性回归模型来说，除了SSE以外，我们还可使用决定系数（R-square，也被称为拟合优度检验）作为其模型评估指标。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948e64a8-6458-4e85-92ad-8477e04f9fbd",
   "metadata": {},
   "source": [
    "决定系数的计算需要使用之前介绍的组间误差平方和和离差平方和的概念。在回归分析中，SSR表示聚类中类似的组间平方和概念，表意为Sum of squares of the regression，由预测数据与标签均值之间差值的平方和计算得出：\n",
    "\n",
    "预测与均值的距离：\n",
    "  $$SSR =\\sum^{n}_{i=1}(\\bar{y_i}-\\hat{y_i})^2$$\n",
    "\n",
    "实际与均值的距离(方差):\n",
    "$$SST =\\sum^{n}_{i=1}(\\bar{y_i}-y_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b090fcdd-ad23-4a87-bfe2-22f3880bdca7",
   "metadata": {},
   "source": [
    "并且，$SST$可由$$SSR+SSE$$计算得出。而决定系数，则由$SSR$和$SST$共同决定：\n",
    "$$R-square=\\frac{SSR}{SST}=\\frac{SST-SSE}{SSE}=1-\\frac{SSE}{SST}$$\n",
    "很明显，决定系数是一个鉴于[0,1]之间的值，并且约趋近于1，模型拟合效果越好。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbb86f2-2b74-491f-81fe-1e97f2743772",
   "metadata": {},
   "source": [
    "**调整R方值**\n",
    "\n",
    "调整后的R方值（Adjusted R²）是在多个自变量的回归分析中更准确地评估模型的拟合优度。与普通R方值不同，调整后的R²会考虑自变量数量，不会因为增加不相关的自变量而人为提高模型的拟合度。\n",
    "$$\\begin{align} \\text{Adjusted } R² = 1 - \\left(\\frac{(1 - R²)(n - 1)}{n - k - 1}\\right) \\end{align}$$其中： $R²$是普通的决定系数；$n$ 是样本数量； $k$ 是自变量的数量。\n",
    "适用场景：在特征数较多时，调整 R² 比普通 R² 更合理。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ea4a5d7-091c-4582-992a-3a6c81a315d9",
   "metadata": {},
   "source": [
    "## 分类Classification\n",
    "\n",
    "predict categories & small number of out puts\n",
    "\n",
    "Binary classification：$y$只能是0和1的其中一个\n",
    "\n",
    "根据肿瘤大小判断是否癌症"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8676eef-8b46-4fdf-bc77-5263e23b1cc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T15:03:37.800491Z",
     "iopub.status.busy": "2025-01-02T15:03:37.800074Z",
     "iopub.status.idle": "2025-01-02T15:03:37.810489Z",
     "shell.execute_reply": "2025-01-02T15:03:37.805246Z",
     "shell.execute_reply.started": "2025-01-02T15:03:37.800435Z"
    }
   },
   "source": [
    "### Sigmod or logistic函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719a26cb-b62a-4a32-a099-a368b8028af5",
   "metadata": {},
   "source": [
    "肿瘤大小(横轴)判定是否癌症(0,1纵轴)，数据分布如图。可以拟合成 曲线，即sigmod函数。\n",
    "$$g(z) = \\frac{1}{1+e^{-z}}\\tag{1}$$\n",
    "\n",
    "![](img_16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79564e7-ac89-407e-ba21-f6ef08ce7f12",
   "metadata": {},
   "source": [
    "在逻辑回归的情况下，z（sigmoid函数的输入）是线性回归模型的输出。 \n",
    "- 在单个示例的情况下，z是标量。\n",
    "- 在多个示例的情况下，z可以是由m个值组成的向量，每个示例一个。 \n",
    "- sigmoid函数的实现应该涵盖这两种潜在的输入格式。\n",
    "\n",
    "让我们在Python中实现这一点。\n",
    "```python\n",
    "def sigmoid(z):\n",
    "    \"\"\"6\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Args:\n",
    "        z (ndarray): A scalar, numpy array of any size.\n",
    "\n",
    "    Returns:\n",
    "        g (ndarray): sigmoid(z), with the same shape as z\n",
    "         \n",
    "    \"\"\"\n",
    "\n",
    "    g = 1/(1+np.exp(-z))\n",
    "   \n",
    "    return g\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ce3eeb-8d91-4183-921c-f5ca7966d161",
   "metadata": {},
   "source": [
    "### 逻辑回归Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d72cc6c-001e-4349-a0e1-bfac913fe22d",
   "metadata": {},
   "source": [
    "A logistic regression model applies the sigmoid to the familiar linear regression model as shown below:\n",
    "$$f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b ) \\tag{2} $$\n",
    "  where$$g(z) = \\frac{1}{1+e^{-z}}\\tag{3}$$\n",
    "Final formula\n",
    "$$f_{w,b}(x^{(i)}) = \\frac1{1 + e^{-wx+b}}\\tag{4}$$\n",
    "输出结果$$f_{w,b}(x^{(i)}) = 0.7$$,\n",
    "代表患癌概率为70%\n",
    "\n",
    "\n",
    "![](img_17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba6c74d-42d9-46bc-b69d-550988209e1f",
   "metadata": {},
   "source": [
    "#### 损失函数Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790ee67d-2de1-43c8-ad68-239037770720",
   "metadata": {},
   "source": [
    "Loss：单个示例与目标值之间差异的指标\n",
    "\n",
    "Cost：训练集损失的衡量标准"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44161ed7-3b72-41b9-a80a-f7e80ef04bed",
   "metadata": {},
   "source": [
    "会导致结果出现多个局部最小值。算法确定全局最小值较为困难\n",
    "      $$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 $$的梯度cost曲线\n",
    "\n",
    "![](img_18.png)\n",
    "      \n",
    "![](img_19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde0cdb8-058e-4563-b474-aa75ae92338a",
   "metadata": {},
   "source": [
    "方法：离散数据连续化\n",
    "$$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$$ 单个示例的cost, 如下:\n",
    "$$\\begin{equation}\n",
    "loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = \\begin{cases}\n",
    "- \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) & \\text{if $y^{(i)}=1$}\\\\\n",
    "\\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) & \\text{if $y^{(i)}=0$}\n",
    "\\end{cases}\n",
    "\\end{equation}$$\n",
    "      $$f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$$ 是预测值, while $y^{(i)}$ 是目标值.\n",
    "      $$f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot\\mathbf{x}^{(i)}+b)$$  $g$ 是 sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd9e624-3104-46da-a6c7-12f085f272a6",
   "metadata": {},
   "source": [
    "该损失函数由两条曲线构成，目标值为1和0时曲线的表现不同。越接近目标，曲线斜率越小，即损失函数对参数更不敏感。\"速度放缓说明越接近真理\",如图:\n",
    "\n",
    "![](img_20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf12ee7-7a2f-4c44-a0a6-fd2562e7e592",
   "metadata": {},
   "source": [
    "#### 损失函数简化版\n",
    "(1)式等价于\n",
    "$$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)$$\n",
    "该式中 $$y^{(i)} = 1 \\space or \\space0$$\n",
    "$$\\begin{align}\n",
    "loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), 0) &= (-(0) \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - 0\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\\\\n",
    "&= -\\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\n",
    "\\end{align}$$\n",
    "$$\\begin{align}\n",
    "loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), 1) &=  (-(1) \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - 1\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\\\\\n",
    "&=  -\\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dcc8c3-74b2-4786-aeb2-18a2fa7d5ecc",
   "metadata": {},
   "source": [
    "下图成本与参数曲线:\n",
    "```python\n",
    "plt.close('all')\n",
    "cst = plt_logistic_cost(x_train,y_train)\n",
    "```\n",
    "![](img_21.png)\n",
    "\n",
    "这条曲线非常适合梯度下降。它没有停滞点(plateau)、局部极小值或不连续点。注意，它不像平方误差那样是碗状的。绘制成本和成本的对数是为了说明这样一个事实，即当成本较小时，曲线有一个斜率并继续下降。\n",
    "假设使用线性回归损失函数模拟\n",
    "单示例下成本函数为:\n",
    "$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 \\tag{1}$$\n",
    "$$f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64a342c-ae4c-4008-b6d9-5460563f34eb",
   "metadata": {},
   "source": [
    "#### 代价函数Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa90300-8d4f-4b86-9211-cd223fd617b2",
   "metadata": {},
   "source": [
    "将所有的loss组合起来，成为成本函数\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{m} \\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] \\tag{1}$$\n",
    "- $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$是单个数据点的cost\n",
    "$$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\tag{2}$$\n",
    "- m是数据集中训练的样本量:\n",
    "$$\\begin{align}\n",
    "f_{\\mathbf{w},b}(\\mathbf{x^{(i)}}) &= g(z^{(i)})\\tag{3} \\\\\n",
    "z^{(i)} &= \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+ b\\tag{4} \\\\\n",
    "g(z^{(i)}) &= \\frac{1}{1+e^{-z^{(i)}}}\\tag{5} \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547b797b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# GBDT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eddb254",
   "metadata": {},
   "source": [
    "如何给5岁孩子解释GBDT\n",
    "\n",
    "一个新手想要做出一道美味佳肴，可以尝试如下步骤\n",
    "\n",
    "1. 第一次做菜时，不懂得调料用量，可以随机放。做完自己尝一尝\n",
    "\n",
    "2. 有了第一次的经验，懂得了控制用量，第二次做的更好吃了\n",
    "\n",
    "3. 经过n次尝试，每次都总结出一些经验，最后实现了这道美味佳肴\n",
    "\n",
    "每次做饭的经验就是**残差**，调料用量就是**超参数**。事先选好的食材、锅具就是**参数**  \n",
    "\n",
    "gbdt就是这个新手"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
